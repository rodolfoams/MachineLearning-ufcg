{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Multivariada com NumPy\n",
    "\n",
    "Autor: Rodolfo Marinho\n",
    "\n",
    "Email: rodolfomarinho (at) copin (dot) ufcg (dot) edu (dot) br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão não vetorizada\n",
    "Função para calcular o MSE (Mean Squared Error):\n",
    "\n",
    "$MSE(\\hat{w}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i (x_i))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_hypothesis(b, m, X):\n",
    "    h_x = b\n",
    "    for i in range(m.size):\n",
    "        h_x += X[i] * m[i]\n",
    "    return h_x\n",
    "\n",
    "def compute_mse(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        X = points[i, :-1]\n",
    "        y = points[i, -1]\n",
    "        totalError += (y - compute_hypothesis(b, m, X))**2\n",
    "    return totalError / float(len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para fazer uma atualização dos parâmetros no Gradiente Descendente:\n",
    "\n",
    "$w_0 = w_0 + 2\\alpha\\sum_{i=1}^N (y_i - (w_0+w_1x_{i_1}+w_2x_{i_2}+\\ldots+w_nx_{i_n}))$\n",
    "\n",
    "$w_1 = w_1 + 2\\alpha\\sum_{i=1}^N x_{i_1}(y_i - (w_0+w_1x_{i_1}+w_2x_{i_2}+\\ldots+w_nx_{i_n}))$\n",
    "\n",
    "...\n",
    "\n",
    "$w_n = w_n + 2\\alpha\\sum_{i=1}^N x_{i_n}(y_i - (w_0+w_1x_{i_1}+w_2x_{i_2}+\\ldots+w_nx_{i_n}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = np.zeros(m_current.size)\n",
    "    for i in range(0, len(points)):\n",
    "        X = points[i, :-1]\n",
    "        y = points[i, -1]\n",
    "        \n",
    "        h_x = compute_hypothesis(b_current, m_current, X)\n",
    "        b_gradient += y - h_x\n",
    "        for j in range(m_gradient.size):\n",
    "            m_gradient[j] += X[j] * (y - h_x)\n",
    "    new_b = b_current + (2 * learningRate * b_gradient)\n",
    "    new_m = np.add(m_current, 2 * learningRate * m_gradient)\n",
    "    return [new_b, new_m, b_gradient, m_gradient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||\\mathbf{w}||_2 = \\sqrt{\\sum_{j=1}^D w_j^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2(x):\n",
    "    c=0\n",
    "    for i in range(len(x)):\n",
    "        c += x[i]**2\n",
    "    return math.sqrt(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para iterar sobre o gradiente descendente até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, epsilon):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    grad = np.array([np.inf]*(starting_m.size + 1))\n",
    "    i = 0\n",
    "    while (norm_2(grad)>=epsilon):\n",
    "        b, m, b_gradient, m_gradient = step_gradient(b, m, points, learning_rate)\n",
    "        grad = [b_gradient]\n",
    "        grad.extend(m_gradient)\n",
    "        grad = np.array(grad)\n",
    "        #print(\"Norm2:\",norm_2(grad))\n",
    "        if i % 1000 == 0:\n",
    "            #print(norm_2(grad))\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse(b,m,points)))\n",
    "        i+= 1\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = [0. 0. 0. 0. 0.], error = 54.47995385562646\n",
      "Running...\n",
      "MSE na iteração 0 é de 33.302186462253474\n",
      "MSE na iteração 1000 é de 0.4300627860129182\n",
      "MSE na iteração 2000 é de 0.42848689972774284\n",
      "MSE na iteração 3000 é de 0.42707721580379643\n",
      "MSE na iteração 4000 é de 0.42578343897705406\n",
      "MSE na iteração 5000 é de 0.42459600894933835\n",
      "MSE na iteração 6000 é de 0.42350618412280805\n",
      "MSE na iteração 7000 é de 0.42250594147706977\n",
      "MSE na iteração 8000 é de 0.42158791747358043\n",
      "MSE na iteração 9000 é de 0.4207453538470714\n",
      "MSE na iteração 10000 é de 0.419972047852873\n",
      "MSE na iteração 11000 é de 0.4192623066038457\n",
      "MSE na iteração 12000 é de 0.41861090516075666\n",
      "MSE na iteração 13000 é de 0.4180130480675691\n",
      "MSE na iteração 14000 é de 0.4174643340484845\n",
      "MSE na iteração 15000 é de 0.4169607236068418\n",
      "MSE na iteração 16000 é de 0.4164985092873406\n",
      "MSE na iteração 17000 é de 0.41607428838267635\n",
      "Gradiente descendente convergiu com b = 0.9237624803254038, m = [0.11762623 0.08368579 0.1622438  0.42250996 0.03029455], erro = 0.415973797840725\n",
      "Versão não vetorizada rodou em: 5325.5040645599365ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "learning_rate = 0.000035\n",
    "initial_b = 0 # initial y-intercept guess\n",
    "initial_m = np.zeros(points[0].size-1) # initial slope guess\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_mse(initial_b, initial_m, points)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "[b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com b = {0}, m = {1}, erro = {2}\".format(b, m, compute_mse(b, m, points)))\n",
    "print(\"Versão não vetorizada rodou em: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão Vetorizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    m_gradient = np.matmul(np.transpose(X),res)\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient))] +\n",
    "             [np.add(np.array(w_current[1:]), (2 * learningRate * m_gradient[1:]))])\n",
    "    return [new_w,b_gradient,m_gradient]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf]*w.size)\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w,b_gradient,m_gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        grad = [b_gradient]\n",
    "        grad.extend(m_gradient)\n",
    "        grad = np.array(grad)\n",
    "        #print(np.linalg.norm(grad))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at W = [0. 0. 0. 0. 0. 0.], error = 54.47995385562645\n",
      "Running...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1a5cdb87263e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_runner_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, error = {2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_mse_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-db11444be9ed>\u001b[0m in \u001b[0;36mgradient_descent_runner_vectorized\u001b[0;34m(starting_w, X, Y, learning_rate, epsilon)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_gradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_gradient_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb_gradient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-b2c6f4eaab15>\u001b[0m in \u001b[0;36mstep_gradient_vectorized\u001b[0;34m(w_current, X, Y, learningRate)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mm_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient))] +\n\u001b[0;32m----> 6\u001b[0;31m              [np.add(np.array(w_current[1:]), (2 * learningRate * m_gradient[1:]))])\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_gradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_gradient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[:,:-1]\n",
    "Y = points[:,-1]\n",
    "init_w = np.zeros((X[0].size))\n",
    "learning_rate = 0.000035\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at W = {0}, error = {1}\".format(init_w, compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, error = {2}\".format(w[0], w[1], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
